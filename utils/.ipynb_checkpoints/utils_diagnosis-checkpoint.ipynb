{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import string\n",
    "import re\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import make_scorer, recall_score, log_loss\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Time Series Analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_seasonal_decomposition(df, col=None, model_=None):\n",
    "    \"\"\"\n",
    "    Perform seasonal decomposition of a time series and return the decomposition results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the time series data.\n",
    "        col (str): The name of the column in the DataFrame to decompose.\n",
    "        model_ (str): The model for decomposition ('additive' or 'multiplicative').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the plot and decomposition results.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = pd.read_csv('time_series_data.csv')\n",
    "        >>> result = perform_seasonal_decomposition(data, col='Value', model_='additive')\n",
    "    \"\"\"\n",
    "    if col is None:\n",
    "        col = df.columns[0]  # If 'col' is not specified, use the first column.\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
    "\n",
    "    # Perform seasonal decomposition\n",
    "    decomposition = seasonal_decompose(df[col], model=model_)\n",
    "\n",
    "    # Plot the original time series, trend, seasonality, and residuals\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(df[col], label='Original')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Original Time Series')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(decomposition.trend, label='Trend')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Trend Component')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonal')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Seasonal Component')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(decomposition.resid, label='Residuals')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    seasonal_decomposition_result = {'plot': plt, 'decomposition': decomposition} \n",
    "               \n",
    "    return seasonal_decomposition_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, roc_auc_score, balanced_accuracy_score\n",
    "import joblib\n",
    "\n",
    "def train_and_score_classifiers(X_train, y_train, X_test, y_test, classifiers, pipeline):\n",
    "    \"\"\"\n",
    "    Train and score multiple classifiers and return performance metrics in a DataFrame.\n",
    "\n",
    "    This function takes training and test data, a set of classifiers, and a scikit-learn pipeline. It trains\n",
    "    each classifier using a RandomizedSearchCV approach and evaluates their performance on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        The training data.\n",
    "\n",
    "    y_train : pd.Series\n",
    "        The target labels for the training data.\n",
    "\n",
    "    X_test : pd.DataFrame\n",
    "        The test data for evaluation.\n",
    "\n",
    "    y_test : pd.Series\n",
    "        The target labels for the test data.\n",
    "\n",
    "    classifiers : dict\n",
    "        A dictionary containing classifiers, their names as keys, and their corresponding parameter grids.\n",
    "        Example: {'RandomForest': {'model': RandomForestClassifier(), 'parameters': {'n_estimators': [10, 100, 500]}}}\n",
    "\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        A scikit-learn pipeline containing data preprocessing steps and a RandomizedSearchCV step.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame containing the following performance metrics for each classifier:\n",
    "        - 'Model': The classifier's name.\n",
    "        - 'Recall': The recall score.\n",
    "        - 'Precision': The precision score.\n",
    "        - 'F1': The F1 score.\n",
    "        - 'Accuracy': The accuracy score.\n",
    "        - 'AUC ROC': The area under the ROC curve.\n",
    "        - 'Balanced Accuracy': The balanced accuracy score.\n",
    "\n",
    "    class_dict : dict\n",
    "        A dictionary containing information about each trained classifier and their predictions.\n",
    "        The structure is {'Classifier Name': {'model': classifier, 'parameters': params, 'X_test': test_data,\n",
    "        'y_test': test_labels, 'predictions': predicted_labels, 'prob_predictions': probability_predictions,\n",
    "        'model_fit': fitted_classifier}}.\n",
    "    \"\"\"\n",
    "    class_dict = {}\n",
    "    df = pd.DataFrame(columns=['Model', 'Recall', 'Precision', 'F1', 'Accuracy', 'AUC ROC', 'Balanced Accuracy'])\n",
    "   \n",
    "    # Preprocess training and test datasets\n",
    "    X_train_pre, X_train_s, y_train_s, groups_train_s, selected_train_cols = pipeline.named_steps['Preprocessor'].fit_transform(X_train, y_train)\n",
    "    X_test_pre, X_test_s, y_test_s, groups_test_s, selected_test_cols = pipeline.named_steps['Preprocessor'].fit_transform(X_test, y_test)\n",
    "\n",
    "    dec = 3\n",
    "    for c in classifiers:\n",
    "        class_dict[c] = {}\n",
    "        classifier = classifiers[c]['model']\n",
    "        params = classifiers[c]['parameters']\n",
    "\n",
    "        print('Training and scoring: ' + str(c))\n",
    "\n",
    "        # Save classifier and params for return\n",
    "        class_dict[c]['model'] = classifier\n",
    "        class_dict[c]['parameters'] = params\n",
    "\n",
    "        # Fit model and test with holdout data\n",
    "        pipeline_fit = pipeline.named_steps['RandomSearchCV'].fit(X_train_s, y_train_s, groups=groups_train_s, base_estimator=classifier, parameters=params)\n",
    "        filename = f'models/{c}_hospitalization_prediction.sav'\n",
    "        joblib.dump(pipeline_fit, filename)\n",
    "        results = pipeline.named_steps['RandomSearchCV'].predict(X_test_s, y_test_s, selected_train_cols)\n",
    "        X_test = results['X_test']\n",
    "        y_test = results['y_test']\n",
    "        predicted = results['predictions']\n",
    "        prob_predictions = results['prob_predictions']\n",
    "        fitted = results['model_fit']\n",
    "        \n",
    "        # Save y_test, predicted, and pipeline for return\n",
    "        class_dict[c]['X_test'] = X_test\n",
    "        class_dict[c]['y_test'] = y_test\n",
    "        class_dict[c]['predictions'] = predicted\n",
    "        class_dict[c]['prob_predictions'] = prob_predictions\n",
    "        class_dict[c]['model_fit'] = fitted\n",
    "\n",
    "        # Score model\n",
    "        recall = recall_score(y_test, predicted)\n",
    "        accuracy = accuracy_score(y_test, predicted)\n",
    "        precision = precision_score(y_test, predicted)\n",
    "        f1 = f1_score(y_test, predicted)\n",
    "        auc_roc = roc_auc_score(y_test, predicted)\n",
    "        bal_acc = balanced_accuracy_score(y_test, predicted)\n",
    "\n",
    "        # append rows to an empty DataFrame\n",
    "        df = df.append({'Model': c, 'Recall': round(recall, dec), 'Precision': round(precision, dec), \\\n",
    "                            'F1': round(f1, dec), 'Accuracy': round(accuracy, dec), 'AUC ROC': round(auc_roc, dec), 'Balanced Accuracy': round(bal_acc, dec)}, ignore_index=True)\n",
    "    return df, class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Custom Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomSelectFromModel(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer for feature selection based on a scikit-learn estimator.\n",
    "\n",
    "    This transformer selects features from the input data using a provided estimator.\n",
    "    It is designed for use within a scikit-learn pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : object, optional\n",
    "        An estimator used for feature selection. It should have a `fit` method.\n",
    "        Default is None.\n",
    "\n",
    "    threshold : float, optional\n",
    "        The threshold for feature selection. Features with importance scores greater than or equal\n",
    "        to this threshold will be selected. Default is None.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(self, X, y=None, base_estimator=None, parameters=None):\n",
    "        Fit the transformer to the data. This method does not perform feature selection but is included\n",
    "        for compatibility within a scikit-learn pipeline.\n",
    "\n",
    "    transform(self, X):\n",
    "        Transform the input data by selecting relevant features using the provided estimator and threshold.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_transformed : pd.DataFrame\n",
    "        The transformed DataFrame with selected features.\n",
    "    y : pd.Series\n",
    "        The target variable (labels).\n",
    "    groups : pd.Series\n",
    "        A group identifier for data grouping or split purposes.\n",
    "    selected_train_cols : list\n",
    "        List of column names that were selected.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> custom_selector = CustomSelectFromModel(estimator=RandomForestClassifier(), threshold=0.2)\n",
    "    >>> X_transformed, y, groups, selected_cols = custom_selector.transform(X)\n",
    "    >>> print(X_transformed.head())\n",
    "    >>> print(selected_cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator=None, threshold=None):\n",
    "        self.estimator = estimator\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y=None, base_estimator=None, parameters=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):   \n",
    "        X_pre = X\n",
    "        X_split, y_split, groups_split = split_data_by_groups(X)\n",
    "        feature_selector = SelectFromModel(self.estimator, threshold=self.threshold)\n",
    "        feature_selector.fit(X_split, y_split) \n",
    "        selected_features = feature_selector.get_support()\n",
    "        selected_train_cols = X_split.columns[selected_features].to_list()\n",
    "        X_split = X_split[selected_train_cols]\n",
    "        return X_pre, X_split, y_split, groups_split, selected_train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomRandomizedSearchCV(BaseEstimator, ClassifierMixin):\n",
    "#     \"\"\"\n",
    "#     Customized Randomized Search Cross-Validation for hyperparameter tuning of a scikit-learn classifier.\n",
    "\n",
    "#     This class extends scikit-learn's BaseEstimator and ClassifierMixin to create a custom Randomized Search CV\n",
    "#     that fits a classifier to the data, performs hyperparameter tuning, and provides predictions.\n",
    "\n",
    "#     Parameters:\n",
    "#     scoring (str or callable, optional): A scoring strategy to evaluate the model's performance during tuning.\n",
    "#         Default is None.\n",
    "#     n_splits (int, optional): The number of cross-validation splits to use. Default is None.\n",
    "\n",
    "#     Attributes:\n",
    "#     scoring (str or callable): The scoring strategy used during hyperparameter tuning.\n",
    "#     n_splits (int): The number of cross-validation splits used for model evaluation.\n",
    "\n",
    "#     Methods:\n",
    "#     fit(X, y=None, base_estimator=None, parameters=None):\n",
    "#         Fit the model to the data, perform hyperparameter tuning, and save the best model.\n",
    "\n",
    "#     predict(X):\n",
    "#         Make predictions on new data using the best-fitted model.\n",
    "\n",
    "#     \"\"\"\n",
    "#     def __init__(self, scoring=None, n_splits=None):\n",
    "#         self.scoring = scoring\n",
    "#         self.n_splits = n_splits\n",
    "    \n",
    "#     def fit(self, X, y=None, base_estimator=None, parameters=None):\n",
    "#         X_train, y_train, groups_train = split_data_by_groups(X)\n",
    "#         cv_splits = GroupTimeSeriesSplit(n_splits=self.n_splits).split(X=X_train, y=y_train, groups=groups_train)\n",
    "#         rscv = RandomizedSearchCV(base_estimator, parameters,scoring=self.scoring,cv=cv_splits)\n",
    "#         rscv.fit(X=X_train, y=y_train)\n",
    "#         filename = 'models/hospitalization_prediction.sav'\n",
    "#         joblib.dump(rscv, filename)\n",
    "    \n",
    "#     def predict(self, X):     \n",
    "#         X_test = X\n",
    "#         X, y, groups = split_data_by_groups(X_test)\n",
    "#         y_test = y.to_numpy()\n",
    "#         filename = 'models/hospitalization_prediction.sav'\n",
    "#         rs_cv = joblib.load(filename)\n",
    "#         predictions = rs_cv.predict(X)\n",
    "#         prob_predictions = rs_cv.predict_proba(X)\n",
    "#         results = {\n",
    "#                     'X_test': X_test, \n",
    "#                     'y_test': y_test, \n",
    "#                     'predictions':predictions, \n",
    "#                     'prob_predictions':prob_predictions, \n",
    "#                     'fitted_rscv': rs_cv\n",
    "#         }\n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomRandomizedSearchCV(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    A custom class for performing randomized hyperparameter tuning and prediction using scikit-learn models.\n",
    "\n",
    "    This class uses RandomizedSearchCV to optimize hyperparameters and then provides prediction functionality.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    scoring : str, optional\n",
    "        The scoring metric for hyperparameter optimization.\n",
    "        Default is None.\n",
    "\n",
    "    n_splits : int, optional\n",
    "        The number of splits to be used in cross-validation.\n",
    "        Default is None.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(self, X, y=None, groups=None, base_estimator=None, parameters=None):\n",
    "        Fit the CustomRandomizedSearchCV to the data by optimizing hyperparameters using RandomizedSearchCV.\n",
    "\n",
    "    predict(self, X, y=None, selected_columns=None):\n",
    "        Make predictions on new data using the trained model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        A dictionary containing the following items:\n",
    "        - 'X_test': The transformed test data.\n",
    "        - 'y_test': The ground truth test labels.\n",
    "        - 'predictions': Predicted labels.\n",
    "        - 'prob_predictions': Predicted probabilities.\n",
    "        - 'fitted_rscv': The fitted RandomizedSearchCV model.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> from sklearn.metrics import accuracy_score\n",
    "    >>> custom_rscv = CustomRandomizedSearchCV(scoring='accuracy', n_splits=5)\n",
    "    >>> custom_rscv.fit(X_train, y_train, groups=group_labels, base_estimator=RandomForestClassifier(), parameters=param_grid)\n",
    "    >>> results = custom_rscv.predict(X_test, y_test, selected_columns)\n",
    "    >>> accuracy = accuracy_score(results['y_test'], results['predictions'])\n",
    "    >>> print(f\"Accuracy: {accuracy}\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scoring=None, n_splits=None):\n",
    "        self.scoring = scoring\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def fit(self, X, y=None, groups=None, base_estimator=None, parameters=None):\n",
    "        \"\"\"\n",
    "        Fit the CustomRandomizedSearchCV to the data by optimizing hyperparameters using RandomizedSearchCV.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            The input data.\n",
    "\n",
    "        y : pd.Series, optional\n",
    "            The target variable (labels).\n",
    "\n",
    "        groups : pd.Series, optional\n",
    "            A group identifier for data grouping or split purposes.\n",
    "\n",
    "        base_estimator : object, optional\n",
    "            The base estimator for hyperparameter optimization.\n",
    "\n",
    "        parameters : dict, optional\n",
    "            Hyperparameter search space.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        cv_splits = GroupTimeSeriesSplit(n_splits=self.n_splits).split(X=X, y=y, groups=groups)\n",
    "        rscv = RandomizedSearchCV(base_estimator, parameters, scoring=self.scoring, cv=cv_splits)\n",
    "        rscv.fit(X=X, y=y)\n",
    "        filename = 'models/hospitalization_prediction.sav'\n",
    "        joblib.dump(rscv, filename)\n",
    "    \n",
    "    def predict(self, X, y=None, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Make predictions on new data using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            The test data for prediction.\n",
    "\n",
    "        y : pd.Series, optional\n",
    "            The ground truth test labels.\n",
    "\n",
    "        selected_columns : list, optional\n",
    "            List of selected columns used for prediction.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            A dictionary containing prediction results and data:\n",
    "            - 'X_test': The transformed test data.\n",
    "            - 'y_test': The ground truth test labels.\n",
    "            - 'predictions': Predicted labels.\n",
    "            - 'prob_predictions': Predicted probabilities.\n",
    "            - 'fitted_rscv': The fitted RandomizedSearchCV model.\n",
    "        \"\"\"\n",
    "        y_test = y.to_numpy()\n",
    "        appended_df = append_new_columns(X, selected_columns)\n",
    "        X_test = appended_df[selected_columns]\n",
    "        filename = 'models/hospitalization_prediction.sav'\n",
    "        rs_cv = joblib.load(filename)\n",
    "        predictions = rs_cv.predict(X_test)\n",
    "        prob_predictions = rs_cv.predict_proba(X_test)\n",
    "        results = {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'predictions': predictions,\n",
    "            'prob_predictions': prob_predictions,\n",
    "            'model_fit': rs_cv\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataFrameMerger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Merge DataFrames using specified parameters.\n",
    "\n",
    "    Args:\n",
    "        right_df (pandas.DataFrame): The DataFrame to merge with.\n",
    "        join_on (str or list): The column(s) to join on.\n",
    "        how_join (str): The type of merge operation (e.g., 'inner', 'left', 'right', 'outer').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, right_df=None, join_on=None, how_join=None):\n",
    "        self.right_df = right_df\n",
    "        self.on = join_on\n",
    "        self.how = how_join\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input DataFrame by merging it with another DataFrame.\n",
    "\n",
    "        Args:\n",
    "            X (pandas.DataFrame): The input DataFrame to merge.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: Transformed DataFrame after merging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.right_df is not None and self.on is not None and self.how is not None:\n",
    "                X = X.merge(self.right_df, on=self.on, how=self.how)\n",
    "                return X\n",
    "            else:\n",
    "                raise ValueError(\"Missing required parameters: right_df, join_on, and how_join.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while merging DataFrames: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetDummiesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer for one-hot encoding categorical features using the\n",
    "    Pandas `get_dummies` function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    *args, **kwargs:\n",
    "        Additional positional and keyword arguments that can be passed to the parent class.\n",
    "    pandas_params : dict, optional (default={})\n",
    "        Additional parameters to pass to the Pandas `get_dummies` function.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    _pandas_params : dict\n",
    "        Additional parameters to pass to the Pandas `get_dummies` function.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(self, X, y=None):\n",
    "        Fit the transformer to the data (no actual fitting is needed in this case).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or DataFrame\n",
    "            The input data.\n",
    "        y : array-like, optional (default=None)\n",
    "            Target variable (ignored, as this transformer does not require target information).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : GetDummiesTransformer\n",
    "            Returns the instance of the transformer.\n",
    "\n",
    "    transform(self, X, y=None):\n",
    "        Transform the input data by applying the Pandas `get_dummies` function.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or DataFrame\n",
    "            The input data to be one-hot encoded.\n",
    "        y : array-like, optional (default=None)\n",
    "            Target variable (ignored, as this transformer does not require target information).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        encoded_data : DataFrame\n",
    "            A DataFrame containing the one-hot encoded representation of the input data.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns=None, prefix=None):\n",
    "        self.columns = columns\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.get_dummies(X, columns=self.columns, prefix=self.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataFrameAggregator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Aggregate data in a DataFrame based on specified grouping columns and aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        grouping_cols (list or str): The column(s) by which data should be grouped.\n",
    "        aggregation (dict): Dictionary specifying the columns and aggregation functions to apply.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with aggregated data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grouping_cols=None, aggregation=None):\n",
    "        self.grouping_cols = grouping_cols\n",
    "        self.aggregation = aggregation\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input DataFrame by aggregating data based on grouping columns and aggregation functions.\n",
    "\n",
    "        Args:\n",
    "            X (pandas.DataFrame): The input DataFrame to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: Transformed DataFrame with aggregated data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.grouping_cols is not None and self.aggregation is not None:\n",
    "                X = X.groupby(self.grouping_cols).agg(self.aggregation).reset_index()\n",
    "                X = X.loc[:,~X.columns.duplicated()].copy() \n",
    "                return X\n",
    "            else:\n",
    "                raise ValueError(\"Missing required parameters: grouping_cols and aggregation.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while aggregating the DataFrame: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible transformer for selecting specific columns from a DataFrame.\n",
    "\n",
    "    This class allows you to specify a list of column names to extract from a DataFrame. It is designed\n",
    "    to work seamlessly with scikit-learn pipelines and transformers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    columns : list\n",
    "        A list of column names to select from the input DataFrame.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    columns : list\n",
    "        The list of column names to be selected.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None):\n",
    "        Fit the transformer to the data. This method does nothing and is included to maintain\n",
    "        scikit-learn compatibility.\n",
    "\n",
    "    transform(X, y=None):\n",
    "        Transform the input DataFrame by selecting the specified columns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            The input DataFrame to select columns from.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        selected_columns : pandas DataFrame\n",
    "            A new DataFrame containing only the specified columns from the input DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for column in self.columns:\n",
    "            if column not in X.columns:\n",
    "                X[column] = 0  # You can set any default value          \n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataframeFunctionTransformer:\n",
    "    \"\"\"\n",
    "    Custom transformer for applying a user-defined function to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    func : callable\n",
    "        A function that takes a DataFrame as input and returns a transformed DataFrame.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    func : callable\n",
    "        The user-defined function to be applied to the input DataFrame.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    transform(input_df, **transform_params):\n",
    "        Applies the user-defined function to the input DataFrame and returns the transformed DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_df : pandas.DataFrame\n",
    "            The input DataFrame to apply the function to.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            A new DataFrame resulting from the application of the user-defined function.\n",
    "\n",
    "    fit(X, y=None, **fit_params):\n",
    "        Fits the transformer to the input data. No actual fitting is performed, and the transformer is returned as is.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            The input DataFrame, not used for fitting.\n",
    "\n",
    "        y : None, optional\n",
    "            Target variable, not used.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "            The fitted transformer instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        return self.func(input_df)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9080253-737a-4394-a73e-6b0093123de9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impute_missing_values_by_group(df, group_key=None, sort_cols=None):\n",
    "    \"\"\"\n",
    "    Impute missing values within groups by forward and backward filling.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        group_key (str): The column name used for grouping data.\n",
    "        sort_cols (list, optional): List of columns by which the DataFrame should be sorted before imputation.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with missing values imputed within groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "    \n",
    "    df = df.sort_values(by=sort_cols) if sort_cols else df\n",
    "\n",
    "    some_cols = df.columns.to_list()\n",
    "    some_cols.remove(group_key)\n",
    "\n",
    "    df[[group_key] + some_cols] = df[[group_key] + some_cols].groupby(df[group_key]).ffill().groupby(df[group_key]).bfill()\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hosp_rate_cluster(df):\n",
    "    \"\"\"\n",
    "    Cluster patients based on their health statistics, especially hospitalization rates.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with patient health statistics.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with an additional 'cluster' column indicating the patient clusters and hospitalization rate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Summarize patient health statistics\n",
    "    patient_stats_df = summarize_patient_health_stats(df)\n",
    "    patient_stats_df = patient_stats_df.replace(np.nan, 0)\n",
    "    data = patient_stats_df[['uid', 'age', 'bp_sys', 'bp_dia', 'hosp_per_visit']].drop('uid', axis=1)\n",
    "    X = StandardScaler().fit_transform(data)\n",
    "\n",
    "    kmeans_results_df = pd.DataFrame(columns=['no_clusters', 'calinski_harabasz_score', 'y_hat'])\n",
    "\n",
    "    C = range(2, 7)\n",
    "    for c in C:\n",
    "        k_means = KMeans(n_clusters=c)\n",
    "        model = k_means.fit(X)\n",
    "        y_hat = k_means.predict(X)\n",
    "\n",
    "        labels = k_means.labels_\n",
    "        cal = metrics.calinski_harabasz_score(X, labels)\n",
    "\n",
    "        kmeans_results_df = kmeans_results_df.append({'no_clusters': c, 'calinski_harabasz_score': cal, 'y_hat': y_hat},\n",
    "                                                      ignore_index=True)\n",
    "\n",
    "    optimal_no_clusters = int(kmeans_results_df[kmeans_results_df.calinski_harabasz_score == kmeans_results_df.calinski_harabasz_score.max()] \\\n",
    "        ['no_clusters'].values[0])\n",
    "\n",
    "    y_hat_optimal = kmeans_results_df[kmeans_results_df.no_clusters == optimal_no_clusters].y_hat.values[0]\n",
    "\n",
    "    c_df = pd.DataFrame(y_hat_optimal, columns=['cluster'])\n",
    "    clusters_df = patient_stats_df.join(c_df)\n",
    "    clusters_mean_df = clusters_df.groupby('cluster').mean()\n",
    "    clusters_mean_df = clusters_mean_df.sort_values(by='hosp_per_visit', ascending=False)\n",
    "    clusters_mean_df = clusters_mean_df.reset_index()\n",
    "\n",
    "    clusters_mean_df = clusters_mean_df.reset_index().rename(columns={'index': 'cluster_new'})\n",
    "\n",
    "    clusters_df = clusters_df.merge(clusters_mean_df[['cluster', 'cluster_new']], left_on='cluster', right_on='cluster').drop('cluster', axis=1).rename(columns={'cluster_new': 'cluster'})\n",
    "\n",
    "    clusters_df = clusters_df[['uid', 'cluster', 'hosp_per_visit']]\n",
    "\n",
    "    df = df.merge(clusters_df, left_on='uid', right_on='uid')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_seasonality(df):\n",
    "    \"\"\"\n",
    "    Add seasonal decomposition data to the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame to which seasonal decomposition data will be added.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The input DataFrame with seasonal decomposition data added.\n",
    "    \n",
    "    Raises:\n",
    "    FileNotFoundError: If the seasonal decomposition data file is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load seasonal decomposition data\n",
    "        seasonal_df = pd.read_parquet('data/seasonal_feature.parquet')\n",
    "        \n",
    "        # Merge the seasonal data into the input DataFrame\n",
    "        df = df.merge(seasonal_df, left_on='visit_date_month', right_on='month').rename(columns={'seasonal':'seasonal_decomp'})\n",
    "        df.drop('month',axis=1, inplace=True)\n",
    "        \n",
    "        df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()   \n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Seasonal decomposition data file 'data/seasonal_feature.parquet' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_hospitalizations_one_week(df):\n",
    "    \"\"\"\n",
    "    Shift hospitalization-related columns in a DataFrame by one week to create previous week statistics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing hospitalization-related data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with additional columns for previous week hospitalization statistics.\n",
    "\n",
    "    \"\"\"\n",
    "    df['ttl_hosp_prev_week'] = df['ttl_hosp_week'].shift(1)\n",
    "    df['ttl_hosp_prev_week'] = df['ttl_hosp_prev_week'].fillna(0)\n",
    "\n",
    "    df['ttl_hosp_count_prev_week'] = df['ttl_hosp_count'].shift(1)\n",
    "    df['ttl_hosp_count_prev_week'] = df['ttl_hosp_count_prev_week'].fillna(0)\n",
    "\n",
    "    df['ttl_visits_prev_week'] = df['ttl_visits'].shift(1)\n",
    "    df['ttl_visits_prev_week'] = df['ttl_visits_prev_week'].fillna(0)\n",
    "\n",
    "    df['hosp_per_visit_prev_week'] = df.ttl_hosp_count_prev_week / df.ttl_visits_prev_week\n",
    "    df['hosp_per_visit_prev_week'] = df['hosp_per_visit_prev_week'].fillna(0)\n",
    "    \n",
    "    df.dropna(inplace=True)   \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def percentage_of_missing_values(df):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of missing values for each column in a DataFrame.\n",
    "\n",
    "    This function computes the percentage of missing (NaN or null) values for each column in the input DataFrame\n",
    "    and returns a DataFrame with two columns: 'column_name' and 'percent_missing'. The resulting DataFrame\n",
    "    is sorted in descending order by the percentage of missing values.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame for which the percentage of missing values should be calculated.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with two columns: 'column_name' and 'percent_missing', indicating the columns and their\n",
    "        respective percentages of missing values, sorted in descending order.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> data = {'A': [1, 2, None, 4, 5], 'B': [None, 2, 3, None, 5]}\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> result = percentage_of_missing_values(df)\n",
    "    >>> print(result)\n",
    "    \n",
    "       column_name  percent_missing\n",
    "    0           A             20.0\n",
    "    1           B             40.0\n",
    "    \"\"\"\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                         'percent_missing': percent_missing.round(2)})\n",
    "    missing_value_df.sort_values('percent_missing', inplace=True, ascending=False)\n",
    "    missing_value_df.reset_index(inplace=True)\n",
    "    return missing_value_df[['column_name','percent_missing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(df, features=None):\n",
    "    \"\"\"\n",
    "    Apply a natural logarithm transformation to specified features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        features (list or str): Name or list of names of the feature(s) to be log-transformed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with the specified feature(s) log-transformed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(features, str):\n",
    "            features = [features]\n",
    "        df[features] = np.log(df[features])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_column_names(df):\n",
    "    \"\"\"\n",
    "    Standardizes the column names of a DataFrame by converting them to lowercase,\n",
    "    replacing punctuation with underscores, and removing leading/trailing underscores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame whose column names need to be standardized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with standardized column names.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pd.DataFrame({'First Name': [1, 2], 'Last Name': [3, 4]})\n",
    "        >>> df = standardize_column_names(df)\n",
    "        >>> print(df.columns)\n",
    "        Index(['first_name', 'last_name'], dtype='object')\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "\n",
    "    for c in df.columns.to_list():\n",
    "        c_mod = c.lower()\n",
    "        c_mod = c_mod.translate(translator)\n",
    "        c_mod = '_'.join(c_mod.split(' '))\n",
    "        if c_mod[-1] == '_':\n",
    "            c_mod = c_mod[:-1]\n",
    "        c_mod = re.sub(r'\\_+', '_', c_mod)\n",
    "        c_mod = re.sub('_+$', '', c_mod)\n",
    "        new_cols.append(c_mod)\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_counts(df):\n",
    "    \"\"\"\n",
    "    Calculate the total count of missing (NaN and None) values in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame for which missing values are counted.\n",
    "\n",
    "    Returns:\n",
    "        int: The total count of missing values in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = {'A': [1, 2, None], 'B': [3, None, 5], 'C': [None, None, None]}\n",
    "        >>> df = pd.DataFrame(data)\n",
    "        >>> missing_count = get_null_counts(df)\n",
    "        >>> print(missing_count)\n",
    "        6\n",
    "    \"\"\"\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    null_count = df.isnull().sum().sum()\n",
    "    total_missing = nan_count + null_count\n",
    "    return total_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_value_cols(df, percent=90):\n",
    "    \"\"\"\n",
    "    Get column names in a DataFrame where a majority of values are the same, excluding binary encoded columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to analyze.\n",
    "    - percent (int, optional): The threshold percentage for considering a column as having similar values.\n",
    "      Columns with more than 'percent' percentage of the same value will be included. Default is 90.\n",
    "\n",
    "    Returns:\n",
    "    - sim_val_cols (list): A list of column names with a majority of similar values.\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    sim_val_cols = []\n",
    "    for col in df.columns:\n",
    "        percent_vals = (df[col].value_counts() / len(df) * 100).values\n",
    "        # filter columns where more than 90% values are same and leave out binary encoded columns\n",
    "        if percent_vals[0] > percent and len(percent_vals) > 2:\n",
    "            sim_val_cols.append(col)\n",
    "            count += 1\n",
    "    print(\"Total columns with majority singular value shares: \", count)\n",
    "    return sim_val_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_lower(df):\n",
    "    \"\"\"\n",
    "    Convert all string values in a DataFrame to lowercase while preserving non-string values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing mixed data types.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame with all string values converted to lowercase, while non-string values remain unchanged.\n",
    "    \"\"\"\n",
    "    df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_outliers(df, columns):\n",
    "    \"\"\"\n",
    "    Detect and remove outliers from a DataFrame.\n",
    "\n",
    "    This function identifies outliers in the specified columns of a DataFrame and removes them.\n",
    "    Outliers are defined as data points that fall outside of the 1.5 * Interquartile Range (IQR)\n",
    "    from the lower and upper quartiles.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame to analyze and clean.\n",
    "    - columns (list): A list of column names in the DataFrame to check for outliers.\n",
    "\n",
    "    Returns:\n",
    "    - df_clean (pandas.DataFrame): The DataFrame with outliers removed.\n",
    "\n",
    "    Example Usage:\n",
    "    cleaned_data = detect_outliers(data_df, ['column1', 'column2'])\n",
    "    \"\"\"\n",
    "\n",
    "    outliers_lst = []\n",
    "    leave_cols = []  # Columns you may want to leave out\n",
    "    # For each feature, find the data points with extreme high or low values\n",
    "    for feature in columns:\n",
    "\n",
    "        if feature not in leave_cols:\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            step = 1.5 * (Q3 - Q1)\n",
    "\n",
    "            # Find outliers\n",
    "            outliers_rows = df.loc[~((df[feature] >= Q1 - step) & (df[feature] <= Q3 + step)), :]\n",
    "            outliers_lst.append(list(outliers_rows.index))\n",
    "\n",
    "    outliers = list(itertools.chain.from_iterable(outliers_lst))\n",
    "    # List of duplicate outliers\n",
    "    dup_outliers = list(set([x for x in tqdm(outliers, position=0) if outliers.count(x) > 1]))\n",
    "    df_clean = df.loc[~df.index.isin(dup_outliers)]\n",
    "    print(\"Processed outliers\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_health_stats_data():\n",
    "    \"\"\"\n",
    "    Load health statistics data from CSV files and merge them into a single DataFrame.\n",
    "\n",
    "    This function reads patient data, visit data, and health statistics data from separate CSV files,\n",
    "    performs necessary data type casting, and then merges them into a single DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing merged health statistics data for patients and visits.\n",
    "    \"\"\"\n",
    "    patients_df = cast_dtypes_patients(pd.read_csv('./data/diagnosis_patients.csv'))  \n",
    "    visits_df = cast_dtypes_visits(pd.read_csv('./data/diagnosis_visits.csv'))  \n",
    "    health_stats_df = cast_dtypes_health_stats(pd.read_csv('./data/diagnosis_health_stats.csv'))\n",
    "    health_stats_patients_df = health_stats_df.merge(patients_df, on=['uid'], how='inner')\n",
    "    health_stats_patients_visits_df = pd.concat([health_stats_patients_df, visits_df])    \n",
    "    return health_stats_patients_visits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_val(n_splits: int,\n",
    "                   splitter_func,\n",
    "                   df: pd.DataFrame,\n",
    "                   title_text: str) -> None:\n",
    "  \n",
    "    \"\"\"Function to plot the cross validation of various\n",
    "    sklearn splitter objects.\"\"\"\n",
    "\n",
    "    split = 1\n",
    "    plot_data = []\n",
    "\n",
    "    for train_index, valid_index in splitter_func:\n",
    "        plot_data.append([train_index, 'Train', f'{split}'])\n",
    "        plot_data.append([valid_index, 'Test', f'{split}'])\n",
    "        split += 1\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data,\n",
    "                           columns=['Index', 'Dataset', 'Split'])\\\n",
    "                           .explode('Index')\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for _, group in plot_df.groupby('Split'):\n",
    "        fig.add_trace(go.Scatter(x=group['Index'].loc[group['Dataset'] == 'Train'],\n",
    "                                 y=group['Split'].loc[group['Dataset'] == 'Train'],\n",
    "                                 name='Train',\n",
    "                                 line=dict(color=\"blue\", width=10)\n",
    "                                 ))\n",
    "        fig.add_trace(go.Scatter(x=group['Index'].loc[group['Dataset'] == 'Test'],\n",
    "                                 y=group['Split'].loc[group['Dataset'] == 'Test'],\n",
    "                                 name='Test',\n",
    "                                 line=dict(color=\"goldenrod\", width=10)\n",
    "                                 ))\n",
    "\n",
    "    fig.update_layout(template=\"simple_white\", font=dict(size=20),\n",
    "                      title_text=title_text, title_x=0.5, width=850,\n",
    "                      height=450, xaxis_title='Index', yaxis_title='Split')\n",
    "\n",
    "    legend_names = set()\n",
    "    fig.for_each_trace(\n",
    "        lambda trace:\n",
    "        trace.update(showlegend=False)\n",
    "        if (trace.name in legend_names) else legend_names.add(trace.name))\n",
    "\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_split_by_year(df, year):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into training and testing sets based on a specified year.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame to be split.\n",
    "    year (int): The year used for splitting the data into training and testing sets.\n",
    "\n",
    "    Returns:\n",
    "    X_train (pandas.DataFrame): The training set containing rows with visit dates before the specified year.\n",
    "    X_test (pandas.DataFrame): The testing set containing rows with visit dates in the specified year.\n",
    "    y_train (None): Placeholder for the training target variable (not used in this function).\n",
    "    y_test (None): Placeholder for the testing target variable (not used in this function).\n",
    "    \"\"\"\n",
    "    X_train = df[df.visit_date_year < year]\n",
    "    y_train = None\n",
    "    X_test = df[df.visit_date_year == year]\n",
    "    y_test = None\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bi_results(health_stats_df, X_test, y_test, best_classifier):\n",
    "    \"\"\"\n",
    "    Prepare results for visualizing health statistics and medications and performing\n",
    "    categorical statistics analysis in a Business Intelligence (BI) tool.\n",
    "\n",
    "    This function processes health statistics data, test data, model predictions, and\n",
    "    probabilities generated by the best classifier to provide structured data for\n",
    "    visualizing health statistics and medications and for performing categorical\n",
    "    statistics analysis in a BI tool.\n",
    "\n",
    "    Parameters:\n",
    "    - health_stats_df (DataFrame): Unprocessed health statistics data.\n",
    "    - X_test (DataFrame): Test data with health statistics features.\n",
    "    - y_test (Series): True labels for the test data.\n",
    "    - best_classifier (dict): A dictionary containing the best classifier with\n",
    "      'model_fit', 'predictions', and 'prob_predictions' keys.\n",
    "\n",
    "    Returns:\n",
    "    - health_stats_results_df (DataFrame): Results for visualizing health statistics,\n",
    "      including predictions, probabilities, and risk classification.\n",
    "    - health_stats_results_meds_df (DataFrame): Results for analyzing medications,\n",
    "      including medication descriptions.\n",
    "    - stats_risk_imp_df (DataFrame): Results for categorical statistics analysis,\n",
    "      imputed for both low and high-risk patient categories.\n",
    "\n",
    "    Example:\n",
    "    health_stats_data = load_health_statistics_data()\n",
    "    X_test, y_test, best_classifier = load_test_data_and_classifier()\n",
    "    health_results, meds_results, stats_results = bi_results(health_stats_data, X_test, y_test, best_classifier)\n",
    "\n",
    "    Notes:\n",
    "    - The 'health_stats_results_df' is structured for visualizing health\n",
    "      statistics and risk classification.\n",
    "    - The 'health_stats_results_meds_df' provides medication information for analysis.\n",
    "    - The 'stats_risk_imp_df' is suitable for analyzing categorical statistics\n",
    "      with imputed values for both low and high-risk patients.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Provide results for all datapoints other than medications and categorical values\n",
    "    X_test_pre, X_test_s, y_test_s, groups_test_s, selected_train_cols = pipeline.named_steps['Preprocessor'].fit_transform(X_test, y_test)\n",
    "    classes = best_classifier['model_fit'].best_estimator_.classes_\n",
    "    predicted = best_classifier['predictions']\n",
    "    prob_predicted = best_classifier['prob_predictions']\n",
    "    predicted_df = pd.DataFrame(predicted, columns = ['predicted'])\n",
    "    prob_predicted_df = pd.DataFrame(prob_predicted, columns = ['no_hosp_prob', 'hosp_prob'])\n",
    "    health_stats_results_df = X_test_pre.join(predicted_df).join(prob_predicted_df)\n",
    "    health_stats_results_df['risk'] = np.where(health_stats_results_df.cluster.isin([0,1]), 'high_risk', 'low_risk')\n",
    "    health_stats_results_df = health_stats_results_df[visit_cols+health_stats_num_cols+['risk','no_hosp_prob', 'hosp_prob','hosp_per_visit', 'predicted']]\n",
    "    health_stats_results_df.age = health_stats_results_df.age.astype(int)\n",
    "    health_stats_results_df[['bp_sys', 'bp_dia', 'weight_lbs', 'temp_f', 'pulse',\n",
    "           'height_val', 'resp_val']] = health_stats_results_df[['bp_sys', 'bp_dia', 'weight_lbs', 'temp_f', 'pulse',\n",
    "           'height_val', 'resp_val']].apply(lambda x: x.round(1))\n",
    "    \n",
    "    # Provide results for analysis of medications\n",
    "    uids_results = health_stats_results_df.uid.unique().tolist()\n",
    "    health_stats_in_results_df = health_stats_df[health_stats_df.uid.isin(uids_results)]\n",
    "    health_stats_results_meds_df = health_stats_in_results_df[['uid', 'med_profile_medication']].drop_duplicates()\n",
    "    health_stats_results_meds_df = health_stats_results_meds_df[~health_stats_results_meds_df.med_profile_medication.isna()]\n",
    "    health_stats_results_meds_df = dataframe_to_lower(health_stats_results_meds_df)\n",
    "    med_descriptions_df = pd.read_csv('data/health_stats_results_meds_desc_l.csv')\n",
    "    health_stats_results_meds_df = health_stats_results_meds_df.merge(med_descriptions_df, left_on='med_profile_medication', right_on='medication')[['uid', 'med_profile_medication','description']]\n",
    "    health_stats_results_meds_df.description = health_stats_results_meds_df.description.fillna('no description available')\n",
    "    \n",
    "    # Provide results for categorical statistics based patient risk category\n",
    "    health_stats_results_stats_df = health_stats_in_results_df.drop('med_profile_medication', axis=1).select_dtypes(object).drop_duplicates()\n",
    "    health_stats_results_stats_df = dataframe_to_lower(health_stats_results_stats_df)\n",
    "    stats_risk_df = health_stats_results_stats_df[health_stats_results_stats_df.uid.isin(uids_results)].merge(health_stats_results_df[['uid','risk']], on='uid', how='left')\n",
    "    stats_risk_lr_df = stats_risk_df[stats_risk_df.risk == 'low_risk']\n",
    "    stats_risk_hr_df = stats_risk_df[stats_risk_df.risk == 'high_risk']\n",
    "    \n",
    "    # Create a SimpleImputer for low risk data\n",
    "    low_risk_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    low_risk_imputer.fit(stats_risk_lr_df)\n",
    "\n",
    "    # Create a SimpleImputer for high risk data\n",
    "    high_risk_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    high_risk_imputer.fit(stats_risk_hr_df)\n",
    "\n",
    "    # Impute missing values in the low risk data\n",
    "    imputed_low_risk_data = low_risk_imputer.transform(stats_risk_lr_df)\n",
    "\n",
    "    # Impute missing values in the high risk data\n",
    "    imputed_high_risk_data = high_risk_imputer.transform(stats_risk_hr_df)\n",
    "\n",
    "    imputed_low_risk_data_df = pd.DataFrame(imputed_low_risk_data, columns=stats_risk_lr_df.columns.to_list()).drop_duplicates()\n",
    "    imputed_high_risk_data_df = pd.DataFrame(imputed_high_risk_data, columns=stats_risk_hr_df.columns.to_list()).drop_duplicates()\n",
    "    stats_risk_imp_df = pd.concat([imputed_low_risk_data_df, imputed_high_risk_data_df])\n",
    "    \n",
    "    return health_stats_results_df, health_stats_results_meds_df, stats_risk_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_new_columns(df, new_columns):\n",
    "    \"\"\"\n",
    "    Appends new columns to a DataFrame to ensure consistency between training and prediction datasets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame to which new columns will be appended.\n",
    "\n",
    "    new_columns : list\n",
    "        A list of column names that need to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        The DataFrame with the new columns added. If a column already exists in the DataFrame, it will be skipped.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> data = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n",
    "    >>> new_columns = ['feature1', 'feature3']\n",
    "    >>> result = append_new_columns(data, new_columns)\n",
    "    >>> print(result)\n",
    "       feature1  feature2  feature3\n",
    "    0         1         4         0\n",
    "    1         2         5         0\n",
    "    2         3         6         0\n",
    "    \"\"\"\n",
    "    for col in new_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_groups(df):\n",
    "    \"\"\"\n",
    "    Split the input DataFrame into feature matrix (X), target variable (y), and groups for cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with the data to be split.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the split data with keys 'X', 'y', and 'groups'.\n",
    "    \n",
    "    Raises:\n",
    "    KeyError: If any required columns are missing in the input DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop unnecessary columns\n",
    "        df = df.drop(columns=['visit_date_week_no', 'ttl_visits', 'ttl_hosp_week', 'ttl_hosp_count'], axis=1)\n",
    "\n",
    "        # Sort the DataFrame\n",
    "        df.sort_values(by=['visit_date_year', 'visit_date_month', 'visit_date_week_dt'], inplace=True)\n",
    "        \n",
    "        # Create groups based on year and month\n",
    "        df_groups = df[['visit_date_year', 'visit_date_month']].drop_duplicates().reset_index().reset_index()[['visit_date_year', 'visit_date_month', 'level_0']].rename(columns={'level_0':'group'})\n",
    "        df = df.merge(df_groups, on=['visit_date_year', 'visit_date_month'])\n",
    "\n",
    "        # Extract features, target, and groups\n",
    "        X = df.drop(columns=['uid', 'hospitalized', 'visit_date_year', 'visit_date_month', 'group', 'visit_date_week_dt'])\n",
    "        y = df.hospitalized.astype(int)\n",
    "        groups = df['group'].to_numpy()\n",
    "        X = X.set_index(groups)\n",
    "\n",
    "        return X, y, groups\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Required columns are missing in the input DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_year_month_groups(df):\n",
    "    \"\"\"\n",
    "    Split the input DataFrame into feature matrix (X), target variable (y), and groups for cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with the data to be split.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the split data with keys 'X', 'y', and 'groups'.\n",
    "    \n",
    "    Raises:\n",
    "    KeyError: If any required columns are missing in the input DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Drop unnecessary columns\n",
    "        df = df.drop(columns=['visit_date_week_no', 'ttl_visits', 'ttl_hosp_week', 'ttl_hosp_count'], axis=1)\n",
    "\n",
    "        # Sort the DataFrame\n",
    "        df.sort_values(by=['visit_date_year', 'visit_date_month', 'visit_date_week_dt'], inplace=True)\n",
    "        \n",
    "        # Create groups based on year and month\n",
    "        df_groups = df[['visit_date_year', 'visit_date_month']].drop_duplicates().reset_index().reset_index()[['visit_date_year', 'visit_date_month', 'level_0']].rename(columns={'level_0':'group'})\n",
    "        df = df.merge(df_groups, on=['visit_date_year', 'visit_date_month'])\n",
    "\n",
    "        # Extract features, target, and groups\n",
    "        X = df.drop(columns=['uid', 'hospitalized', 'visit_date_year', 'visit_date_month', 'group', 'visit_date_week_dt'])\n",
    "        y = df.hospitalized.astype(int)\n",
    "        groups = df['group'].to_numpy()\n",
    "        X = X.set_index(groups)\n",
    "\n",
    "        return X, y, groups\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Required columns are missing in the input DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_dtypes_health_stats(df):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame containing health statistics and medication data by performing the following operations:\n",
    "\n",
    "    1. Remove the 'med_profile_medication' field, which causes duplicates in the data, and deduplicate the DataFrame based on 'uid' and 'visit_date'.\n",
    "    2. Dummy encode medication data to remove duplication.\n",
    "    3. Filter medications based on their correlation with hospitalizations.\n",
    "    4. Rejoin medications with other health statistics in the original DataFrame.\n",
    "    5. Cast specific columns to the correct data types.\n",
    "    6. Convert date-related columns to their appropriate data types.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame containing health statistics and medication data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preprocessed_df : pandas DataFrame\n",
    "        A preprocessed DataFrame with the specified operations applied.\n",
    "    \"\"\"\n",
    "    # Cast specific columns to float data type\n",
    "    df[['bp_sys', 'bp_dia', 'weight_lbs', 'temp_f', 'pulse', 'height_val', 'resp_val']] = \\\n",
    "    df[['bp_sys', 'bp_dia', 'weight_lbs', 'temp_f', 'pulse', 'height_val', 'resp_val']].astype('float')\n",
    "\n",
    "    # Convert date-related columns to appropriate data types\n",
    "    df.visit_date_year = df.visit_date_year.astype(int)\n",
    "    df.visit_date_month = df.visit_date_month.astype(int)\n",
    "    df.visit_date_week_no = df.visit_date_week_no.astype(int)\n",
    "    df.visit_date_week_dt = pd.to_datetime(df.visit_date_week_dt)\n",
    "    df.visit_date = pd.to_datetime(df.visit_date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_dtypes_patients(df):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame containing patient data by performing several operations:\n",
    "    \n",
    "    3. Convert the 'age' column to integer data type.\n",
    "    4. Convert the 'p_uid' and 'gender' columns to string data type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame containing patient data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preprocessed_df : pandas DataFrame\n",
    "        A preprocessed DataFrame with duplicate rows removed, patients under 18 filtered out,\n",
    "        'age' column converted to integer, and 'p_uid' and 'gender' columns converted to string.\n",
    "    \"\"\"\n",
    "    # Convert 'age' to integer\n",
    "    df.age = df.age.astype(int)\n",
    "    \n",
    "    # Convert 'p_uid' and 'gender' to string\n",
    "    df[['uid', 'gender']] = df[['uid', 'gender']].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cast_dtypes_visits(df):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame containing visit data by performing several operations:\n",
    "\n",
    "    1. Remove duplicate rows from the DataFrame.\n",
    "    2. Convert the 'visit_date_year', 'visit_date_month', and 'visit_date_week_no' columns to strings.\n",
    "    3. Convert the 'visit_date_week_dt' column to a datetime field.\n",
    "    4. Convert 'ttl_visits', 'ttl_hosp_week', 'ttl_hosp_count', and 'hospitalized' columns to integers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame containing visit data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    preprocessed_df : pandas DataFrame\n",
    "        A preprocessed DataFrame with duplicate rows removed, data type conversions for date-related columns,\n",
    "        and integer conversion for relevant numerical columns.\n",
    "    \"\"\"\n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Convert year, month, and week_no to strings\n",
    "    df.visit_date_year = df.visit_date_year.astype(int)\n",
    "    df.visit_date_month = df.visit_date_month.astype(int)\n",
    "    df.visit_date_week_no = df.visit_date_week_no.astype(int)\n",
    "\n",
    "    # Convert 'visit_date_week_dt' to datetime\n",
    "    df.visit_date_week_dt = pd.to_datetime(df.visit_date_week_dt)\n",
    "\n",
    "    # Convert selected columns to integers\n",
    "    df[['ttl_visits', 'ttl_hosp_week', 'ttl_hosp_count', 'hospitalized']] = \\\n",
    "    df[['ttl_visits', 'ttl_hosp_week', 'ttl_hosp_count', 'hospitalized']].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_patient_health_stats(df):\n",
    "    \"\"\"\n",
    "    Summarize patient health-related statistics and hospitalization information for each patient.\n",
    "\n",
    "    This function takes a DataFrame containing patient health-related data, including unique patient\n",
    "    identifiers ('uid'), and computes summary statistics for various health metrics such as age, total score,\n",
    "    wound number, systolic and diastolic blood pressure, weight, temperature, pulse rate, height, and respiration rate.\n",
    "    Additionally, it calculates the ratio of hospitalizations to total visits for each patient.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing patient health data, including 'uid', 'age', 'total_score',\n",
    "      'wound_number', 'bp_sys' (systolic blood pressure), 'bp_dia' (diastolic blood pressure), 'weight_lbs', 'temp_f'\n",
    "      (temperature in Fahrenheit), 'pulse' (pulse rate), 'height_val' (height value), 'resp_val' (respiration value),\n",
    "      'ttl_hosp_count' (total hospitalizations), and 'ttl_visits' (total visits).\n",
    "\n",
    "    Returns:\n",
    "    - df_patient_stats (pandas.DataFrame): A DataFrame summarizing the health statistics and hospitalization ratio for each patient.\n",
    "      It includes the 'uid', mean values for the specified health metrics, and 'hosp_per_visit' (hospitalizations per visit).\n",
    "\n",
    "    Example Usage:\n",
    "    df_patient_stats = summarize_patient_health_stats(patient_data_df)\n",
    "    \"\"\"\n",
    "\n",
    "    df_mean_health_stats = df[['uid', 'age', 'bp_sys', 'bp_dia', 'weight_lbs',\n",
    "        'temp_f', 'pulse', 'height_val', 'resp_val']].groupby('uid').mean().reset_index()\n",
    "    \n",
    "    df_hosp_count = df[['uid', 'ttl_hosp_count', 'ttl_visits']].groupby('uid').max().reset_index()\n",
    "    df_hosp_count['hosp_per_visit'] = df_hosp_count.ttl_hosp_count / df_hosp_count.ttl_visits\n",
    "    df_hosp_count = df_hosp_count[['uid', 'hosp_per_visit']]\n",
    "    df_hosp_count.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "    df_patient_stats = df_mean_health_stats.merge(df_hosp_count, left_on='uid', right_on='uid')\n",
    "    df_patient_stats.hosp_per_visit.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "    df_patient_stats.hosp_per_visit = df_patient_stats.hosp_per_visit.fillna(0)\n",
    "    return df_patient_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils_diagnosis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
